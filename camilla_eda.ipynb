{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Camilla's Exploratory Graphs and XGBoost Model"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from xgboost import XGBRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.feature_selection import SelectPercentile, f_regression\n","from sklearn.preprocessing import PolynomialFeatures, StandardScaler, OneHotEncoder\n","mergdf = pd.read_csv(\"merged3.csv\")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>count</th>\n","      <th>name</th>\n","      <th>tempmax</th>\n","      <th>tempmin</th>\n","      <th>temp</th>\n","      <th>feelslikemax</th>\n","      <th>feelslikemin</th>\n","      <th>feelslike</th>\n","      <th>dew</th>\n","      <th>...</th>\n","      <th>description</th>\n","      <th>icon</th>\n","      <th>stations</th>\n","      <th>full_moon</th>\n","      <th>Holiday</th>\n","      <th>Day of Week</th>\n","      <th>CHIC917URN</th>\n","      <th>day_of_week</th>\n","      <th>year</th>\n","      <th>month</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2010-01-01</td>\n","      <td>13.0</td>\n","      <td>Chicago,United States</td>\n","      <td>-8.3</td>\n","      <td>-14.5</td>\n","      <td>-11.7</td>\n","      <td>-15.0</td>\n","      <td>-22.4</td>\n","      <td>-19.0</td>\n","      <td>-16.1</td>\n","      <td>...</td>\n","      <td>Partly cloudy throughout the day.</td>\n","      <td>partly-cloudy-day</td>\n","      <td>72534014819,KORD,KMDW,72530094846,74466504838,...</td>\n","      <td>NaN</td>\n","      <td>New Year's Day</td>\n","      <td>Friday</td>\n","      <td>12.2</td>\n","      <td>4</td>\n","      <td>2010</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2010-01-02</td>\n","      <td>4.0</td>\n","      <td>Chicago,United States</td>\n","      <td>-11.4</td>\n","      <td>-16.4</td>\n","      <td>-14.1</td>\n","      <td>-18.4</td>\n","      <td>-25.3</td>\n","      <td>-22.6</td>\n","      <td>-19.6</td>\n","      <td>...</td>\n","      <td>Partly cloudy throughout the day.</td>\n","      <td>partly-cloudy-day</td>\n","      <td>72534014819,KORD,KMDW,72530094846,74466504838,...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>12.2</td>\n","      <td>5</td>\n","      <td>2010</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2 rows Ã— 41 columns</p>\n","</div>"],"text/plain":["         date  count                   name  tempmax  tempmin  temp  \\\n","0  2010-01-01   13.0  Chicago,United States     -8.3    -14.5 -11.7   \n","1  2010-01-02    4.0  Chicago,United States    -11.4    -16.4 -14.1   \n","\n","   feelslikemax  feelslikemin  feelslike   dew  ...  \\\n","0         -15.0         -22.4      -19.0 -16.1  ...   \n","1         -18.4         -25.3      -22.6 -19.6  ...   \n","\n","                         description               icon  \\\n","0  Partly cloudy throughout the day.  partly-cloudy-day   \n","1  Partly cloudy throughout the day.  partly-cloudy-day   \n","\n","                                            stations  full_moon  \\\n","0  72534014819,KORD,KMDW,72530094846,74466504838,...        NaN   \n","1  72534014819,KORD,KMDW,72530094846,74466504838,...        NaN   \n","\n","          Holiday  Day of Week  CHIC917URN  day_of_week  year  month  \n","0  New Year's Day       Friday        12.2            4  2010      1  \n","1             NaN          NaN        12.2            5  2010      1  \n","\n","[2 rows x 41 columns]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["mergdf.head(2)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["X = mergdf[['tempmax',\n","            'tempmin',\n","            'temp',\n","            'feelslikemax',\n","            'feelslikemin',\n","            'feelslike',\n","            'humidity',\n","            'precip',\n","            'precipprob',\n","            'precipcover',\n","            'snow',\n","            'snowdepth',\n","            'windgust',\n","            'windspeed',\n","            'winddir', \n","            'sealevelpressure',\n","            'cloudcover',\n","            'visibility',\n","            'solarradiation',\n","            'solarenergy',\n","            'uvindex',\n","            'severerisk', \n","            'moonphase',\n","            'day_of_week',\n","            'year',\n","            'month',\n","            'preciptype',\n","            'sunrise',\n","            'sunset',\n","            'conditions',\n","            'full_moon', \n","            'Holiday',\n","            'CHIC917URN']]\n","\n","y = list(mergdf['count'])"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["#num_features = mergdf[['tempmax', 'tempmin', 'temp', 'feelslikemax', 'feelslikemin', 'feelslike', 'humidity',\n","#                        'precip', 'precipprob', 'precipcover', 'snow', 'snowdepth', 'windgust', 'windspeed', 'winddir', \n","#                        'sealevelpressure', 'cloudcover', 'visibility', 'solarradiation', 'solarenergy', 'uvindex', 'severerisk', \n","#                        'moonphase', 'day_of_week', 'year', 'month', 'CHIC917URN']]\n","\n","#cat_features = mergdf[['preciptype', 'conditions', 'full_moon', 'Holiday']]\n","\n","cat_features = X.select_dtypes(include=\"object\").columns\n","num_features = X.select_dtypes(exclude=\"object\").columns\n","\n","numeric_transformer = Pipeline(\n","    steps=[(\"imputer\", SimpleImputer()),\n","           ('poly', PolynomialFeatures(degree=1, include_bias=False)),\n","           (\"scaler\", StandardScaler())])\n","\n","categorical_transformer = Pipeline(\n","    steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n","           (\"encoder\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")),\n","           (\"selector\", SelectPercentile(f_regression, percentile=50))])"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["preprocessor = ColumnTransformer(\n","    transformers=[\n","        (\"num\", numeric_transformer, num_features),\n","        (\"cat\", categorical_transformer, cat_features)])\n","\n","pipe = Pipeline(\n","    steps=[(\"preprocessor\", preprocessor), \n","           (\"model\", XGBRegressor())])"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","\n","\n","xgb_pipe = pipe.fit(X_train, y_train)\n"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean Squared Error: 4.5164189565022355\n","Mean Absolute Error: 1.684635028746969\n"]}],"source":["preds = xgb_pipe.predict(X_test)\n","\n","# Evaluate the model\n","mse = mean_squared_error(y_test, preds)\n","print(\"Mean Squared Error:\", mse)\n","\n","mae = mean_absolute_error(y_test, preds)\n","print(\"Mean Absolute Error:\", mae)\n"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"data":{"text/plain":["3.9454861782331334"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["np.mean(mergdf['count'])"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"All arrays must be of the same length","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[41], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m feature_importance \u001b[39m=\u001b[39m xgb_pipe\u001b[39m.\u001b[39mnamed_steps[\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfeature_importances_\n\u001b[1;32m      6\u001b[0m \u001b[39m# Create a DataFrame with feature names and their importance scores\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m feature_importance_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mFeature\u001b[39m\u001b[39m'\u001b[39m: X_train\u001b[39m.\u001b[39mcolumns, \u001b[39m'\u001b[39m\u001b[39mImportance\u001b[39m\u001b[39m'\u001b[39m: feature_importance})\n\u001b[1;32m      9\u001b[0m \u001b[39m# Sort features by importance\u001b[39;00m\n\u001b[1;32m     10\u001b[0m feature_importance_df \u001b[39m=\u001b[39m feature_importance_df\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mImportance\u001b[39m\u001b[39m'\u001b[39m, ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n","File \u001b[0;32m~/anaconda3/envs/new386/lib/python3.11/site-packages/pandas/core/frame.py:736\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    730\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    731\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    732\u001b[0m     )\n\u001b[1;32m    734\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    735\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 736\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy, typ\u001b[39m=\u001b[39mmanager)\n\u001b[1;32m    737\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    738\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m \u001b[39mimport\u001b[39;00m mrecords\n","File \u001b[0;32m~/anaconda3/envs/new386/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39mdtype, typ\u001b[39m=\u001b[39mtyp, consolidate\u001b[39m=\u001b[39mcopy)\n","File \u001b[0;32m~/anaconda3/envs/new386/lib/python3.11/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    115\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n","File \u001b[0;32m~/anaconda3/envs/new386/lib/python3.11/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"]}],"source":["import matplotlib.pyplot as plt\n","\n","# Get feature importance scores from the trained XGBoost model\n","feature_importance = xgb_pipe.named_steps['model'].feature_importances_\n","\n","# Create a DataFrame with feature names and their importance scores\n","feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importance})\n","\n","# Sort features by importance\n","feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n","\n","# Plot feature importance\n","plt.figure(figsize=(10, 6))\n","plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n","plt.xlabel('Importance')\n","plt.ylabel('Feature')\n","plt.title('Feature Importance')\n","plt.show()"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"All arrays must be of the same length","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[45], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m feature_importance \u001b[39m=\u001b[39m xgb_pipe\u001b[39m.\u001b[39mnamed_steps[\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfeature_importances_\n\u001b[1;32m      4\u001b[0m \u001b[39m# Create a DataFrame with feature names and their importance scores\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m feature_importance_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mFeature\u001b[39m\u001b[39m'\u001b[39m: X_test\u001b[39m.\u001b[39mcolumns, \u001b[39m'\u001b[39m\u001b[39mImportance\u001b[39m\u001b[39m'\u001b[39m: feature_importance})\n\u001b[1;32m      7\u001b[0m \u001b[39m# Sort features by importance\u001b[39;00m\n\u001b[1;32m      8\u001b[0m feature_importance_df \u001b[39m=\u001b[39m feature_importance_df\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mImportance\u001b[39m\u001b[39m'\u001b[39m, ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n","File \u001b[0;32m~/anaconda3/envs/new386/lib/python3.11/site-packages/pandas/core/frame.py:736\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    730\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    731\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    732\u001b[0m     )\n\u001b[1;32m    734\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    735\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 736\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy, typ\u001b[39m=\u001b[39mmanager)\n\u001b[1;32m    737\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    738\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m \u001b[39mimport\u001b[39;00m mrecords\n","File \u001b[0;32m~/anaconda3/envs/new386/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39mdtype, typ\u001b[39m=\u001b[39mtyp, consolidate\u001b[39m=\u001b[39mcopy)\n","File \u001b[0;32m~/anaconda3/envs/new386/lib/python3.11/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    115\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n","File \u001b[0;32m~/anaconda3/envs/new386/lib/python3.11/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"]}],"source":["# Get feature importance scores from the trained XGBoost model\n","feature_importance = xgb_pipe.named_steps['model'].feature_importances_\n","\n","# Create a DataFrame with feature names and their importance scores\n","feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importance})\n","\n","# Sort features by importance\n","feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n","\n","feature_importance_df"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["year: 0.03809408807207769\n","feelslikemin: 0.03050054540186331\n","tempmin: 0.02912704351002332\n","month: 0.02727346423202409\n","solarradiation: 0.02632996890147137\n","solarenergy: 0.019375020583889534\n","precip: 0.011534872943480346\n","tempmax: 0.010709759130245878\n","CHIC917URN: 0.00974769134110769\n","day_of_week: 0.0075520290450932556\n","windgust: 0.006898744965198433\n","sealevelpressure: 0.003638282005031146\n","precipcover: 0.003350985000806983\n","conditions: 0.003231655202908479\n","Holiday: 0.002453605348519372\n","snowdepth: 0.001987280095064048\n","winddir: 0.0017322281957616648\n","severerisk: 0.001487875128237992\n","humidity: 0.0006711123646561679\n","precipprob: 0.0\n","sunrise: 0.0\n","sunset: 0.0\n","full_moon: 0.0\n","preciptype: 0.0\n","windspeed: -0.00015814006860195295\n","uvindex: -0.000252198396502179\n","visibility: -0.00039631303069030953\n","temp: -0.0005885853457081369\n","cloudcover: -0.001206906465935209\n","feelslike: -0.001516175281833909\n","snow: -0.0017567517255823173\n","moonphase: -0.005235031463223583\n","feelslikemax: -0.0068264603149829515\n"]}],"source":["from sklearn.inspection import permutation_importance\n","\n","# Compute permutation feature importance\n","perm_importance = permutation_importance(xgb_pipe, X_test, y_test, n_repeats=10, random_state=42)\n","\n","# Get feature names\n","feature_names = X_test.columns\n","\n","# Calculate mean importance and sort by descending order\n","mean_importance = perm_importance['importances_mean']\n","sorted_indices = mean_importance.argsort()[::-1]\n","\n","# Print feature importance\n","for idx in sorted_indices:\n","    print(f\"{feature_names[idx]}: {mean_importance[idx]}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["A higher number for permutation importance indicates that the feature is more important for the model's predictive performance. Therefore, you would typically want to keep features with higher permutation importance values, as they contribute more to the model's accuracy or predictive power."]}],"metadata":{"kernelspec":{"display_name":"new386","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
